4 Fine-tuning
Fine-tuning PLMs in low-resource scenarios poses
challenges like overfitting and unstable optimization due to the limited amount of data compared
to the huge model capacity, which inhibits generalization to unseen examples.
Catastrophic forgetting mitigation. To effectively fine-tune deeper transformer models with
small data, proper optimization and regularization
techniques are essential (Kalajdzievski, 2024).
Zhang et al. (2021) explores the effect of training length and the varying number of re-initialized
higher layers on BERT fine-tuning. Xu et al.
(2021a) propose an input-based weight-scaling
strategy to stabilize training and speed up the convergence of deep models with low data. They also
empirically show that large batch size has a negative impact on generalization with small datasets.
Layer-wise learning rate decay (LLRD) applies
higher learning rates deeper into the network with
the goal of preserving more general information
from the pre-trained network and learning taskspecific information in the last layers (Howard
and Ruder, 2018). Regularization strategies mitigate catastrophic forgetting by encouraging parameters to remain close to their pre-trained values. Pre-trained weight decay applies a constraint on all parameters (Wiese et al., 2017), while
Mixout (Lee et al., 2019) stochastically replaces
some of the weights of the model with the pretrained weights. SMART (Jiang et al., 2020a) incorporates smoothness-inducing adversarial regularization for robustness to small perturbations and
Bregman proximal point optimization to avoid aggressive parameter updates outside a trust region.
Besides optimization and regularization, incorporating external knowledge can also be very effective. Phang et al. (2019) propose fine-tuning first
on a larger, intermediate task before moving on to
the target task with limited data.
4.1 Parameter-efficient training
Fine-tuning the entire set of parameters (millions
or billions) in PLMs is sample-inefficient and can
be unstable in low-resource settings (Dodge et al.,
2020). Parameter-efficient fine-tuning (PEFT)
methods update only a reduced set of weights,
avoiding high computational costs. They are particularly useful in data-scarce scenarios because
they mitigate the risk of catastrophic forgetting
associated with full re-training. moreover,some methods even managed to match the performance
of full fine-tuning with only a small fraction of the
parameters.
Masking-based methods do not add additional
parameters but train only a subset of the model
weights (specific layers, parameter types, etc.)
while keeping the rest fixed. Early work simply
trains (additional) last layers on top of encoder
models (Devlin et al., 2019). BitFit (Ben Zaken
et al., 2022) tunes the bias terms of the model,
making it highly efficient. Another line of work
focuses on subnetwork optimization based on gradient information, either by choosing a fixed subnetwork before training (Xu et al., 2021b; Ansell
et al., 2022), or selecting it adaptively at train time
using a multi-stage optimization strategy (Zhang
et al., 2022; Yu et al., 2023). Masking-based methods are efficient and simple to implement; however, they tend to underperform compared to other
PEFT methods that add new trainable parameters
(Liu et al., 2022; Mao et al., 2022).
Adapters are trainable lightweight feedforward
modules that are injected between the transformer
layers, while the rest of the model is fixed
(Houlsby et al., 2019; Pfeiffer et al., 2020). Sequential adapters act as a bottleneck in the model,
requiring wider layers and more parameters compared to other PEFT methods to maintain performance (Hu et al., 2023b). Compacter (Karimi Mahabadi et al., 2021) alleviates this issue by utilizing low-rank matrices and parameter sharing.
Adapters reduce training time and memory consumption but have a negative impact on inference
time due to the additional model depth (Rücklé
et al., 2021). Parallel adapters (He et al., 2021) and
Ladder Side-Tuning (Sung et al., 2022) mitigate
this by incorporating learnable modules in parallel
to the backbone model. Multiple adapters can also
be flexibly combined to handle complex tasks in a
modular fashion (Pfeiffer et al., 2021; Wang et al.,
2022; Chronopoulou et al., 2023).Prefix-tuning builds upon the idea of in-context
learning, but instead of finding discrete tokens,
it optimizes continuous embeddings to serve as a
task-specific context for the model. Specifically,
the method prepends learned token vectors to the
input key and values of multi-head attention layers in each transformer block, acting as virtual tokens to attend to (Li and Liang, 2021). IA3
instead introduces learned vectors to scale the keys and
values in attention mechanisms and feed-forward
networks, showing improved performance (comparable to adapters) with an order of magnitude
more parameters (Liu et al., 2022). Prompt-tuning
(Lester et al., 2021) further reduces the number
of parameters by limiting the prefix to the input
embeddings. This method performs competitively
with very large model sizes (billions of parameters) and shows slower convergence (Mao et al.,
2022). Overall, prefix-tuning offers a flexible and
efficient way to adapt models in cross-domain and
cross-lingual low-resource scenarios (Tu et al.,
2024; Goswami et al., 2023; Zhao et al., 2022)
with considerably less parameters than adapters or
reparametrization methods.
Reparametrization methods take inspiration
from the observation that parameters of LLMs
reside on a low-dimensional manifold. Intrinsic SAID (Aghajanyan et al., 2021) investigates
the intrinsic dimensionality and projects the additive weight matrices into this subspace. Similarly, LoRA (Hu et al., 2021) decomposes the
weight matrices into the product of two low-rank
matrices, significantly reducing parameter count
without compromising much performance. KronA
(Edalati et al., 2022) replaces the Kronecker decomposition and shows better downstream performance. Reparametrization methods work best for
large weight matrices and, thus, large models with
moderate or greater amounts of data (Van Veen
et al., 2023).
Hybrid methods combine multiple PEFT
methods to leverage their individual strengths.
UniPELT (Mao et al., 2022) uses a gating
mechanism to dynamically activate adapters,
prefix-tuning, and LoRA to optimize performance
for a given task and data setup. Chen et al.
(2022a) introduce design spaces to parametrize
layer grouping, trainable parameter allocation,
and PEFT strategy selection.4.2 Embedding learning
Embedding vectors are the numerical representations of input tokens in NLP tasks. They are crucial for the success of LLMs in downstream tasks,
enabling the model to capture the semantic information of the input text, including the nuances
of the specific language, domain, and task (Collobert et al., 2011). However, language models are
limited to a predefined vocabulary resulting fromthe tokenization stage of fixed granularity. Choosing the granularity implies a trade-off in time and
space complexity. Word-level granularity is more
expressive but requires a larger vocabulary and
more memory, while character- or byte-level granularity is more space-efficient but less expressive
and produces very long sequences (Bojanowski
et al., 2017; Ruder et al., 2023). As a compromise,
most models use subword-level granularity, which
is often a good balance between the two (Kudo and
Richardson, 2018). The best granularity depends
on the task and language. A fixed vocabulary is
sensitive to small textual perturbations and limits generalization to new tasks and domains. To
address this, Sun et al. (2023) propose training a
shallow transformer to learn word representations
from characters, making it robust to spelling errors
and domain shifts. Recent work trains the token
embeddings with frozen transformer body, to address the discrepancy between the pre-trained and
target domain and language vocabulary (Artetxe
et al., 2020; Hung et al., 2023). This approach
is much more efficient than full fine-tuning and
can act as an intermediate training step. Alternatively, Nag et al. (2023) identify words vulnerable to fragmentation based on the entropy
of the token embeddings and augment the vocabulary with new embeddings for these words.
Another approach to enhance cross-lingual transfer is to map the embeddings for different languages into a shared space using parallel data or
seed dictionaries (Mikolov et al., 2013; Lalrempuii and Soni, 2023). Leveraging high-resource
language embeddings this way can be highly beneficial for low-resource downstream tasks (Minixhofer et al., 2022; Deshpande et al., 2022; Deb
et al., 2023). Other works establish an implicit
mapping through alignment training objectives on
paired data (Cao et al., 2019; Saadi et al., 2022).4.3 Contrastive and adversarial learning
Contrastive and adversarial learning methods extract meaningful information from the differences
and similarities across languages and domains, enhancing model alignment and adaptation.
Contrastive learning (CL) aims to learn effective representations by pulling semantically close
pairs together and pushing apart unrelated samples (Chen et al., 2020b). It usually requires parallel data and can happen on multiple levels of
granularity: sentence (Chi et al., 2021a) and word (Chi et al., 2021b; Chen et al., 2023b). Alignment
is significantly correlated with cross-lingual transfer across different languages and models (Gaschi
et al., 2023), achieving remarkable performance in
downstream tasks with labeled examples only in
the source language (Hu et al., 2023a; Kowsher
et al., 2023). A number of other works highlight
the synergy of contrastive cross-linguality with
PEFT methods like adapters (Liu et al., 2023b,a;
Ahmat et al., 2023). Besides cross-lingual alignment, CL can also be beneficial at the downstream
task level to make better use of the data. For
text similarity tasks, it can act as an unsupervised objective by generating positive pairs with
data augmentation techniques (Gao et al., 2021b;
Yan et al., 2021). CL pairs can be easily created for binary classification tasks, like machinegenerated text detection (Liu et al., 2023e), and for
other classification tasks, like sentiment analysis
or Named Entity Recognition (NER), by anchoring to the textual description of the class (Pauli
et al., 2023) or reformulating as a question answering (QA) task (Chen et al., 2023c).
Adversarial learning refers to training two
models simultaneously with contradicting objectives, guiding each other towards better performance (Goodfellow et al., 2014). Adversarial
training can help bridge the gap between the pretraining and target domain or language without
any paired data. The key mechanism is training
a language or domain discriminator that needs to
be deceived by the model, forcing it to learn robust representations that are domain-invariant (Du
et al., 2020; Grießhaber et al., 2020) or languageagnostic (Lange et al., 2020; Huang et al., 2023c).
This can also be combined with PEFT methods
like adapters Ngo Trung et al. (2021) or learning
prefixes that are either specific or independent of
the domain Zhao et al. (2022).4.4 Limited supervision
In low-resource scenarios, semi-supervised, unsupervised, and active learning methods can successfully leverage unlabeled data to boost model generalization and robustness.
Semi-supervised learning (SSL) leverages both
labeled and unlabeled data during the training process (Chapelle et al., 2009). One common approach is self-training, where a model is trained on
labeled data, and its predictions on unlabeled data
are treated as pseudo-labels for additional training mostly based on model confidence (Schick and
Schütze, 2021a; Wang et al., 2023b; Lalrempuii
and Soni, 2023), or entropy (Chen et al., 2020a).
However, to avoid confirmation bias, proper regularization is essential (Toivanen et al., 2022). Unlike standard methods, consistency regularization
promotes stable predictions on perturbed inputs
(Sohn et al., 2020; Xie et al., 2020; Li et al., 2019).
Another common SSL approach is co-training,
where multiple modules are trained on different
views of the input data and either predict pseudolabels for each other or need to agree on the final
prediction (Clark et al., 2018; Bhattacharjee et al.,
2020). SSL has been particularly useful in transfer
learning approaches, where language models can
profit from the pre-trained knowledge and generate better pseudo-labels for the target task.
Unsupervised methods train models using only
unlabeled data, making them especially useful in
scenarios where labeled data is scarce or unavailable. The most prominent example and a key factor in the success of PLMs is their self-supervised
pre-training objectives (§ 3), which can also be
employed for the pre-training of PEFT modules
like adapters (Diao et al., 2023). On the other
hand, unsupervised methods during fine-tuning
capitalize on some form of consistency condition. Besides consistency regularization (§ 4.4),
which involves perturbing the input, the relation
of different modalities can also be described using cycle-consistency (Zhu et al., 2017). Unpaired
data is required to learn bidirectional relationships
between potentially disparate domains (Karisani,
2022; Buehler, 2023), languages (Lample et al.,
2018; Ren et al., 2019), or even text styles (Jalota
et al., 2023). In practice, unsupervised objectives
require a large amount of unlabeled data to learn
meaningful representations of the data distributions. Therefore, they are often combined with direct supervision to improve performance.Active learning (AL) techniques focus on selecting the most informative data points to maximize the effectiveness of limited training data.
This method assumes a special scenario with unlabeled data and a constrained annotation budget,
a situation that is common in real-world applications (Ren et al., 2021). High model uncertainty through metrics like confidence scores, entropy, Monte Carlo dropout, and perplexity are the most prominent data sampling criteria (Lewis and
Gale, 1994; Gal and Ghahramani, 2016; Houlsby
et al., 2011; Yuan et al., 2020; Muradoglu and
Hulden, 2022; Jantscher et al., 2023). These can
be combined with diversity-based sampling strategies – based on data distribution similarity or gradient variance – to ensure balanced data representation and mitigate outliers (Sener and Savarese,
2018; Gissin and Shalev-Shwartz, 2018; Ash et al.,
2019; Ein-Dor et al., 2020; Margatina et al., 2021;
Karamcheti et al., 2021; François and Gay, 2023).
Training models with AL is an iterative process
where a small batch of unlabeled samples is selected for annotation based on the progressively
refined model. Re-initializing the model between
rounds is more stable than incrementally updating it with new data, especially for underrepresented classes (Lemmens and Daelemans, 2023).
For the cold start problem, an alternative to random sampling is to use a self-supervised objective
as a surrogate for uncertainty (Yuan et al., 2020).
Integrating AL with PEFT techniques, such as
adapters and UniPELT, has shown promising results in boosting performance for low-resource
tasks (Jukic and Snajder ´ , 2023). Overall, AL
strategies allow models to learn efficiently from
limited data, reducing the annotation burden and
adjusting it to the task complexity.