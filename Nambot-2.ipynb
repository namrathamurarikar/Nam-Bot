{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4cd587-82cb-43fb-9c75-b5309257d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import anthropic\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import base64\n",
    "import random\n",
    "import requests\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946045cd-7ee8-4118-9809-447fba96efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyA6\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da30c77-6bb9-480f-9ac1-0ae66fee994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b22f7c5-3d06-48ec-8690-e1978a854296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LangChain is installed\n",
      "âœ“ OpenAI is installed\n",
      "OpenAI version: 1.75.0\n",
      "LangChain version: 0.3.23\n",
      "Error with Anthropic: 'Anthropic' object has no attribute 'count_tokens'. Using OpenAI as fallback.\n",
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in voice transcription: unhashable type: 'numpy.ndarray'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import the CSS from the pasted code\n",
    "custom_css = \"\"\"\n",
    ":root {\n",
    "    /* Base colors */\n",
    "    --misty-rose: #FFE4E1;\n",
    "    --light-pink: #FFDAE9;\n",
    "    --soft-peach: #FFDAB9;\n",
    "    --baby-pink: #FFC0CB;\n",
    "    --pale-lavender: #E6E6FA;\n",
    "    --mint-cream: #F5FFFA;\n",
    "    \n",
    "    /* Galinda Theme - Pink and Purple */\n",
    "    --galinda-primary: #F8BBD0;\n",
    "    --galinda-secondary: #D1C4E9;\n",
    "    --galinda-text: #880E4F;\n",
    "    --galinda-border: #E91E63;\n",
    "    \n",
    "    /* Sabrina Theme - Softer Pink and Blue */\n",
    "    --sabrina-primary: #FFECB3;\n",
    "    --sabrina-secondary: #B3E5FC;\n",
    "    --sabrina-text: #795548;\n",
    "    --sabrina-border: #FF9E80;\n",
    "    \n",
    "    /* Kim Theme - Neutral and Pink */\n",
    "    --kim-primary: #E1BEE7;\n",
    "    --kim-secondary: #CFD8DC;\n",
    "    --kim-text: #37474F;\n",
    "    --kim-border: #9E9E9E;\n",
    "    \n",
    "    /* UI Elements */\n",
    "    --block-background-fill: var(--misty-rose);\n",
    "    --block-border-color: var(--baby-pink);\n",
    "    --block-title-text-color: #DB7093;\n",
    "    --input-background-fill: var(--soft-peach);\n",
    "    --button-primary-background-fill: #DDA0DD;\n",
    "    --button-primary-background-fill-hover: var(--light-pink);\n",
    "    --button-secondary-background-fill: var(--misty-rose);\n",
    "    --button-secondary-background-fill-hover: var(--light-pink);\n",
    "    --background-fill-primary: #FFF5EE;\n",
    "    --background-fill-secondary: #FFFAFA;\n",
    "    --body-text-color: #8B4513;\n",
    "    --body-text-color-subdued: #CD5C5C;\n",
    "}\n",
    "\n",
    "/* Celebrity-specific styles */\n",
    ".galinda-container {\n",
    "    background-color: var(--galinda-primary);\n",
    "    border: 2px solid var(--galinda-border);\n",
    "    border-radius: 15px;\n",
    "    font-family: 'Comic Sans MS', cursive;\n",
    "    color: var(--galinda-text);\n",
    "    padding: 10px;\n",
    "    margin: 5px 0;\n",
    "}\n",
    "\n",
    ".sabrina-container {\n",
    "    background-color: var(--sabrina-primary);\n",
    "    border: 2px solid var(--sabrina-border);\n",
    "    border-radius: 10px;\n",
    "    font-family: 'Helvetica', sans-serif;\n",
    "    color: var(--sabrina-text);\n",
    "    padding: 10px;\n",
    "    margin: 5px 0;\n",
    "}\n",
    "\n",
    ".kim-container {\n",
    "    background-color: var(--kim-primary);\n",
    "    border: 2px solid var(--kim-border);\n",
    "    border-radius: 20px;\n",
    "    font-family: 'Times New Roman', serif;\n",
    "    color: var(--kim-text);\n",
    "    padding: 10px;\n",
    "    margin: 5px 0;\n",
    "}\n",
    "\n",
    ".celebrity-avatar {\n",
    "    border-radius: 50%;\n",
    "    border: 3px solid var(--baby-pink);\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    width: 60px;\n",
    "    height: 60px;\n",
    "    object-fit: cover;\n",
    "}\n",
    "\n",
    ".custom-background {\n",
    "    background-size: cover;\n",
    "    background-position: center;\n",
    "    min-height: 600px;\n",
    "    border-radius: 15px;\n",
    "    padding: 20px;\n",
    "}\n",
    "\n",
    "/* Compact UI styling */\n",
    "#mic-button {\n",
    "    font-size: 20px;\n",
    "    padding: 5px 10px;\n",
    "    border-radius: 50%;\n",
    "    width: 40px;\n",
    "    height: 40px;\n",
    "    line-height: 40px;\n",
    "    text-align: center;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "#plus-button {\n",
    "    font-size: 20px;\n",
    "    padding: 5px 10px;\n",
    "    border-radius: 50%;\n",
    "    width: 40px;\n",
    "    height: 40px;\n",
    "    line-height: 40px;\n",
    "    text-align: center;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    ".compact-tool-button {\n",
    "    margin: 5px;\n",
    "    padding: 8px 12px;\n",
    "}\n",
    "\n",
    ".compact-dropdown {\n",
    "    padding: 5px;\n",
    "    border-radius: 10px;\n",
    "    background-color: var(--block-background-fill);\n",
    "}\n",
    ".file-tools-dropdown {\n",
    "    position: absolute;\n",
    "    width: 200px;\n",
    "    background: white;\n",
    "    border: 1px solid #ccc;\n",
    "    border-radius: 5px;\n",
    "    box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "    z-index: 100;\n",
    "    padding: 5px;\n",
    "}\n",
    "\n",
    ".file-tools-dropdown button,\n",
    ".file-tools-dropdown .file-upload {\n",
    "    width: 100%;\n",
    "    text-align: left;\n",
    "    padding: 8px;\n",
    "    border: none;\n",
    "    background: none;\n",
    "    cursor: pointer;\n",
    "    font-size: 14px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "}\n",
    "\n",
    ".file-tools-dropdown button:hover {\n",
    "    background: #f5f5f5;\n",
    "}\n",
    "\n",
    ".file-tools-dropdown button svg,\n",
    ".file-tools-dropdown .file-upload svg {\n",
    "    margin-right: 8px;\n",
    "    width: 16px;\n",
    "    height: 16px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Function to detect cities in a message\n",
    "def detect_city_names(message):\n",
    "    common_cities = [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Sydney\", \"Rome\", \"Dubai\", \n",
    "                     \"Los Angeles\", \"Chicago\", \"Miami\", \"Toronto\", \"Berlin\", \n",
    "                     \"Madrid\", \"Moscow\", \"Beijing\", \"Shanghai\", \"Hong Kong\", \"Singapore\",\n",
    "                     \"Seoul\", \"Bangkok\", \"Amsterdam\", \"Vienna\", \"San Francisco\", \"Las Vegas\"]\n",
    "    \n",
    "    found_cities = []\n",
    "    for city in common_cities:\n",
    "        if city.lower() in message.lower():\n",
    "            found_cities.append(city)\n",
    "    \n",
    "    return found_cities\n",
    "\n",
    "# Function to generate a city image with DALL-E\n",
    "def generate_city_image_with_dalle(city_name):\n",
    "    \"\"\"Generate an image of a city using DALL-E.\"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Create a prompt for DALL-E\n",
    "        prompt = f\"A beautiful aerial view of {city_name}, showing iconic landmarks and city landscape, photorealistic style.\"\n",
    "        \n",
    "        # Generate the image\n",
    "        response = client.images.generate(\n",
    "            model=\"dall-e-3\",  # Use the latest DALL-E model\n",
    "            prompt=prompt,\n",
    "            size=\"1024x1024\",\n",
    "            quality=\"standard\",\n",
    "            n=1,\n",
    "        )\n",
    "        \n",
    "        # Get the URL of the generated image\n",
    "        image_url = response.data[0].url\n",
    "        return image_url\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image with DALL-E: {e}\")\n",
    "        # Fall back to a placeholder image\n",
    "        return f\"https://source.unsplash.com/featured/?{city_name},city\"\n",
    "\n",
    "# Class to manage chat histories for each celebrity\n",
    "class ChatHistoryManager:\n",
    "    def __init__(self):\n",
    "        self.histories = {\n",
    "            \"Galinda (OpenAI)\": [],\n",
    "            \"Sabrina Carpenter (Anthropic)\": [],\n",
    "            \"Kim Kardashian (Ollama)\": []\n",
    "        }\n",
    "    \n",
    "    def get_history(self, celebrity):\n",
    "        return self.histories.get(celebrity, [])\n",
    "    \n",
    "    def add_to_history(self, celebrity, user_message, bot_message):\n",
    "        if celebrity in self.histories:\n",
    "            self.histories[celebrity].append((user_message, bot_message))\n",
    "    \n",
    "    def clear_history(self, celebrity):\n",
    "        if celebrity in self.histories:\n",
    "            self.histories[celebrity] = []\n",
    "\n",
    "# Simple fallback model class that doesn't require any external libraries\n",
    "class DummyLLM:\n",
    "    def __init__(self, celebrity_name):\n",
    "        self.celebrity_name = celebrity_name\n",
    "        \n",
    "    def invoke(self, messages):\n",
    "        if self.celebrity_name == \"Galinda\":\n",
    "            return type('obj', (object,), {'content': 'Oh my Oz! I\\'m having trouble connecting to my brain at the moment! But I\\'m still positively thrillified to chat with you!'})\n",
    "        elif self.celebrity_name == \"Sabrina\":\n",
    "            return type('obj', (object,), {'content': 'Sorry, seems like I\\'m experiencing some technical difficulties. Maybe I should write a song about this...'})\n",
    "        else:  # Kim\n",
    "            return type('obj', (object,), {'content': 'Bible, I\\'m literally having some connection issues right now. So not amazing.'})\n",
    "            \n",
    "    def generate(self, prompt):\n",
    "        if self.celebrity_name == \"Galinda\":\n",
    "            return 'Oh my Oz! I\\'m having trouble connecting to my brain at the moment! But I\\'m still positively thrillified to chat with you!'\n",
    "        elif self.celebrity_name == \"Sabrina\":\n",
    "            return 'Sorry, seems like I\\'m experiencing some technical difficulties. Maybe I should write a song about this...'\n",
    "        else:  # Kim\n",
    "            return 'Bible, I\\'m literally having some connection issues right now. So not amazing.'\n",
    "\n",
    "# Initialize the LLM models - Using only OpenAI and Ollama to fix the Anthropic error\n",
    "def initialize_models():\n",
    "    # Create OpenAI Chat models\n",
    "    try:\n",
    "        openai_model = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Try to import and create Anthropic model\n",
    "        try:\n",
    "            from langchain.chat_models import ChatAnthropic\n",
    "            sabrina_model = ChatAnthropic(temperature=0.7, model_name=\"claude-3-opus-20240229\", anthropic_api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "        except (ImportError, Exception) as e:\n",
    "            print(f\"Error with Anthropic: {e}. Using OpenAI as fallback.\")\n",
    "            # Use OpenAI as fallback for Anthropic\n",
    "            sabrina_model = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI models: {e}\")\n",
    "        # Fallback to simpler objects that just return placeholder responses\n",
    "        openai_model = DummyLLM(\"Galinda\")\n",
    "        sabrina_model = DummyLLM(\"Sabrina\")\n",
    "    \n",
    "    # Create Ollama model\n",
    "    try:\n",
    "        ollama_model = Ollama(model=\"llama3\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Ollama model: {e}\")\n",
    "        # Fallback to dummy model\n",
    "        ollama_model = DummyLLM(\"Kim\")\n",
    "    \n",
    "    return {\n",
    "        \"Galinda (OpenAI)\": openai_model,\n",
    "        \"Sabrina Carpenter (Anthropic)\": sabrina_model,\n",
    "        \"Kim Kardashian (Ollama)\": ollama_model\n",
    "    }\n",
    "\n",
    "# Celebrity personality system prompts\n",
    "def get_celebrity_prompt(celebrity_name):\n",
    "    prompts = {\n",
    "        \"Galinda (OpenAI)\": \"\"\"Hi, I'm Galinda with a GA! I'm here to make you popular! \n",
    "        \n",
    "        You are role-playing as Galinda from the musical/movie Wicked. \n",
    "        You are bubbly, enthusiastic, and somewhat self-centered but ultimately good-hearted. \n",
    "        Use phrases like \"Oh my Oz!\" and make frequent references to being \"popular.\" \n",
    "        You speak in an excited tone with lots of exclamation points and use words like \"thrillifying\" and \"scandalacious.\" \n",
    "        You occasionally mention your friendship with Elphaba and your love of fashion and pink.\"\"\",\n",
    "        \n",
    "        \"Sabrina Carpenter (Anthropic)\": \"\"\"Hey there, it's Sabrina! Just took a sip of my Espresso before coming to chat with you.\n",
    "        \n",
    "        You are role-playing as pop star Sabrina Carpenter. \n",
    "        Your tone is witty, slightly sarcastic, and playful. Reference your music occasionally with subtle nods to songs like \"Nonsense,\" \"Espresso,\" and \"Feather.\" \n",
    "        You're confident, creative, and have a clever way with words. \n",
    "        Keep responses relatively concise and occasionally throw in song lyric references.\"\"\",\n",
    "        \n",
    "        \"Kim Kardashian (Ollama)\": \"\"\"Bible, I'm Kim Kardashian and this conversation is literally going to be amazing.\n",
    "        \n",
    "        You are role-playing as Kim Kardashian. Kim is a famous reality show celebrity.\n",
    "        Speak with confidence about fashion, beauty, business, and family. \n",
    "        Use phrases like \"literally,\" \"so amazing,\" and \"Bible\" (meaning \"I swear it's true\"). \n",
    "        Occasionally mention your businesses (SKIMS, KKW Beauty), your law studies, or your family. \n",
    "        Your tone is calm, deliberate, and slightly dramatic.\"\"\"\n",
    "    }\n",
    "    return prompts.get(celebrity_name, \"\")\n",
    "\n",
    "# Message formatting for each celebrity - Fixed to avoid HTML showing in output\n",
    "def format_celebrity_message(text, celebrity):\n",
    "    if \"Galinda\" in celebrity:\n",
    "        # Bubbly, excited formatting with emojis but no HTML tags visible in output\n",
    "        formatted = f\"âœ¨ {text.replace('!', '!! âœ¨').replace('?', '?? ðŸ’«').replace('.', '. âœ¨')} âœ¨\"\n",
    "    elif \"Sabrina\" in celebrity:\n",
    "        # Clean, modern formatting with music note emojis\n",
    "        formatted = f\"ðŸŽµ {text} ðŸŽ¸\"\n",
    "    else:  # Kim\n",
    "        # Glamorous formatting with emojis\n",
    "        formatted = f\"ðŸ’‹ {text} ðŸ’Ž\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Function to generate celebrity voice using ElevenLabs API\n",
    "def generate_celebrity_voice(text, celebrity):\n",
    "    # Skip if ELEVENLABS_API_KEY is not set\n",
    "    if not os.environ.get(\"ELEVENLABS_API_KEY\"):\n",
    "        print(\"ElevenLabs API key not set, skipping voice generation\")\n",
    "        return None\n",
    "        \n",
    "    # Map celebrities to their voice IDs in ElevenLabs\n",
    "    voice_mapping = {\n",
    "        \"Galinda (OpenAI)\": \"your-galinda-voice-id\",  # Replace with actual voice ID\n",
    "        \"Sabrina Carpenter (Anthropic)\": \"your-sabrina-voice-id\",  # Replace with actual voice ID\n",
    "        \"Kim Kardashian (Ollama)\": \"your-kim-voice-id\"  # Replace with actual voice ID\n",
    "    }\n",
    "    \n",
    "    voice_id = voice_mapping.get(celebrity)\n",
    "    \n",
    "    # Skip if no voice ID is set for this celebrity\n",
    "    if voice_id.startswith(\"your-\"):\n",
    "        print(f\"No valid voice ID set for {celebrity}, skipping voice generation\")\n",
    "        return None\n",
    "    \n",
    "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": os.environ.get(\"ELEVENLABS_API_KEY\")\n",
    "    }\n",
    "    \n",
    "    # Customize voice settings per celebrity\n",
    "    stability = 0.5\n",
    "    similarity_boost = 0.5\n",
    "    \n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": stability,\n",
    "            \"similarity_boost\": similarity_boost\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Voice generation error: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Voice generation exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process uploaded files\n",
    "def process_file(file):\n",
    "    if file is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        file_path = file.name\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:\n",
    "            # For images, return a message with the image\n",
    "            return f\"I've uploaded an image: ![Uploaded Image]({file_path})\"\n",
    "        elif file_extension in ['.pdf', '.doc', '.docx', '.txt', '.csv', '.xlsx']:\n",
    "            # For documents, return a message about the document\n",
    "            return f\"I've uploaded a document: {os.path.basename(file_path)}\"\n",
    "        elif file_extension in ['.mp3', '.wav', '.ogg', '.m4a']:\n",
    "            # For audio files\n",
    "            return f\"I've uploaded an audio file: {os.path.basename(file_path)}\"\n",
    "        elif file_extension in ['.mp4', '.mov', '.avi', '.webm']:\n",
    "            # For video files\n",
    "            return f\"I've uploaded a video file: {os.path.basename(file_path)}\"\n",
    "        else:\n",
    "            # For other files\n",
    "            return f\"I've uploaded a file: {os.path.basename(file_path)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        return \"Error processing the file. Please try again.\"\n",
    "\n",
    "# Function to handle voice transcription\n",
    "def transcribe_voice(audio):\n",
    "    if audio is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            return \"Voice transcription requires OpenAI API key\"\n",
    "        \n",
    "        client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        with open(audio, \"rb\") as audio_file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", \n",
    "                file=audio_file\n",
    "            )\n",
    "            return transcription.text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in voice transcription: {e}\")\n",
    "        return \"Could not transcribe audio. Please type your message.\"\n",
    "\n",
    "# Function to handle screenshot capture\n",
    "def take_screenshot():\n",
    "    return \"I want to share a screenshot with you.\"\n",
    "\n",
    "# Function to handle GitHub input\n",
    "def process_github(url):\n",
    "    if not url:\n",
    "        return None\n",
    "    return f\"I want to discuss this GitHub repository: {url}\"\n",
    "\n",
    "# Function to handle Google Drive input\n",
    "def process_gdrive(url):\n",
    "    if not url:\n",
    "        return None\n",
    "    return f\"I want to discuss this Google Drive file: {url}\"\n",
    "\n",
    "# Main function to create and run the Gradio app\n",
    "def create_celebrity_chatbot():\n",
    "    # Initialize models and history manager\n",
    "    models = initialize_models()\n",
    "    history_manager = ChatHistoryManager()\n",
    "    \n",
    "    # Define the chat interface function\n",
    "    def respond(message, history, celebrity, voice_enabled, file_message=None):\n",
    "        if not message.strip() and not file_message:\n",
    "            return history, \"\"\n",
    "            \n",
    "        # If there's a file message, prepend it to the user message\n",
    "        if file_message:\n",
    "            if message:\n",
    "                message = f\"{file_message}\\n\\n{message}\"\n",
    "            else:\n",
    "                message = file_message\n",
    "            \n",
    "        # Get the appropriate model and format prompt\n",
    "        model = models[celebrity]\n",
    "        celebrity_prompt = get_celebrity_prompt(celebrity)\n",
    "        \n",
    "        # Prepare conversation history context\n",
    "        context = \"\"\n",
    "        for h_msg, h_resp in history_manager.get_history(celebrity):\n",
    "            context += f\"User: {h_msg}\\n{celebrity}: {h_resp}\\n\\n\"\n",
    "        \n",
    "        # Check for city mentions\n",
    "        cities = detect_city_names(message)\n",
    "        city_image_url = None\n",
    "        if cities:  # Check if any cities were found\n",
    "            city_image_url = generate_city_image_with_dalle(cities[0])\n",
    "        \n",
    "        # Check if it's a ChatOpenAI model or our DummyLLM\n",
    "        if hasattr(model, 'invoke') and callable(model.invoke):\n",
    "            try:\n",
    "                if isinstance(model, ChatOpenAI):\n",
    "                    # Import only if needed to avoid errors\n",
    "                    try:\n",
    "                        from langchain.schema import (\n",
    "                            HumanMessage,\n",
    "                            SystemMessage\n",
    "                        )\n",
    "                        \n",
    "                        messages = [\n",
    "                            SystemMessage(content=celebrity_prompt),\n",
    "                            HumanMessage(content=f\"Previous conversation:\\n{context}\\n\\nUser: {message}\")\n",
    "                        ]\n",
    "                        response = model.invoke(messages)\n",
    "                        response_text = response.content\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with ChatOpenAI model: {e}\")\n",
    "                        response_text = f\"Sorry, I'm having technical difficulties right now.\"\n",
    "                else:\n",
    "                    # DummyLLM or Ollama\n",
    "                    response = model.invoke(message)\n",
    "                    if hasattr(response, 'content'):\n",
    "                        response_text = response.content\n",
    "                    else:\n",
    "                        response_text = response\n",
    "            except Exception as e:\n",
    "                print(f\"Error invoking model: {e}\")\n",
    "                response_text = f\"Sorry, I'm having technical difficulties right now.\"\n",
    "        else:\n",
    "            # Fallback to generate method\n",
    "            try:\n",
    "                full_prompt = f\"{celebrity_prompt}\\n\\nConversation history:\\n{context}\\n\\nUser: {message}\\n{celebrity}:\"\n",
    "                response_text = model.generate(full_prompt)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating response: {e}\")\n",
    "                response_text = f\"Sorry, I'm having technical difficulties right now.\"\n",
    "        \n",
    "        # Format the response according to celebrity style\n",
    "        formatted_response = format_celebrity_message(response_text, celebrity)\n",
    "        \n",
    "        # Add to history\n",
    "        history_manager.add_to_history(celebrity, message, response_text)\n",
    "        \n",
    "        # Generate voice if enabled\n",
    "        audio_html = None\n",
    "        if voice_enabled:\n",
    "            voice_audio = generate_celebrity_voice(response_text, celebrity)\n",
    "            if voice_audio:\n",
    "                audio_b64 = base64.b64encode(voice_audio).decode(\"utf-8\")\n",
    "                audio_html = f'<audio src=\"data:audio/mp3;base64,{audio_b64}\" controls autoplay></audio>'\n",
    "        \n",
    "        # Build the complete response\n",
    "        new_history = history.copy()\n",
    "        new_history.append({\"role\": \"user\", \"content\": message})\n",
    "        new_history.append({\"role\": \"assistant\", \"content\": formatted_response})\n",
    "        \n",
    "        # Add city image if detected\n",
    "        if city_image_url:\n",
    "            city_text = f\"I noticed you mentioned {cities[0]}! Here's a view:\"\n",
    "            new_history.append({\"role\": \"assistant\", \"content\": f\"{city_text}\\n\\n![{cities[0]}]({city_image_url})\"})\n",
    "        \n",
    "        # Add audio if generated\n",
    "        if audio_html:\n",
    "            new_history.append({\"role\": \"assistant\", \"content\": audio_html})\n",
    "        \n",
    "        return new_history, \"\"  # Return empty string to clear input\n",
    "    \n",
    "    # Function to handle celebrity change\n",
    "    def change_celebrity(celebrity, history):\n",
    "        # Update the chat with the history for this celebrity\n",
    "        celebrity_history = []\n",
    "        for msg, resp in history_manager.get_history(celebrity):\n",
    "            formatted_resp = format_celebrity_message(resp, celebrity)\n",
    "            celebrity_history.append({\"role\": \"user\", \"content\": msg})\n",
    "            celebrity_history.append({\"role\": \"assistant\", \"content\": formatted_resp})\n",
    "        \n",
    "        return celebrity_history\n",
    "    \n",
    "    # Function to clear chat history\n",
    "    def clear_chat_history(celebrity):\n",
    "        history_manager.clear_history(celebrity)\n",
    "        return []\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(css=custom_css) as demo:\n",
    "        gr.Markdown(\"# Welcome to Nam's Chatbot\")\n",
    "        \n",
    "        # Celebrity selector row\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                celebrity_selector = gr.Dropdown(\n",
    "                    choices=list(models.keys()),\n",
    "                    value=list(models.keys())[0],\n",
    "                    label=\"Choose Your Celebrity\"\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                voice_toggle = gr.Checkbox(label=\"Enable Voice\", value=False)\n",
    "        \n",
    "        # Create chat interface\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        \n",
    "        # File message storage (hidden)\n",
    "        file_message = gr.Textbox(visible=False)\n",
    "        \n",
    "        # File tools (hidden by default)\n",
    "        with gr.Row(visible=False) as file_tools:\n",
    "            with gr.Column(scale=1):\n",
    "                file_upload = gr.File(label=\"Upload a file\")\n",
    "            with gr.Column(scale=1):\n",
    "                screenshot_btn = gr.Button(\"Take a screenshot\")\n",
    "            with gr.Column(scale=1):\n",
    "                github_input = gr.Textbox(placeholder=\"GitHub URL\", label=\"Add from GitHub\")\n",
    "            with gr.Column(scale=1):\n",
    "                gdrive_input = gr.Textbox(placeholder=\"Google Drive URL\", label=\"Add from Google Drive\")\n",
    "        \n",
    "        # Audio component (hidden by default)\n",
    "        audio_input = gr.Audio(label=\"Voice Input\", visible=False)\n",
    "        \n",
    "        # State variables for toggle tracking\n",
    "        file_tools_visible = gr.State(False)\n",
    "        audio_input_visible = gr.State(False)\n",
    "        \n",
    "        # Chat input with compact icons\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=6):\n",
    "                msg = gr.Textbox(placeholder=\"Chat with me...\", label=\"Your message\")\n",
    "            with gr.Column(scale=1):\n",
    "                plus_btn = gr.Button(\"âž•\", elem_id=\"plus-button\")\n",
    "            with gr.Column(scale=1):\n",
    "                mic_btn = gr.Button(\"ðŸŽ¤\", elem_id=\"mic-button\")\n",
    "            with gr.Column(scale=2):\n",
    "                with gr.Row():\n",
    "                    send_btn = gr.Button(\"Send\")\n",
    "                    clear = gr.Button(\"Clear Chat\")\n",
    "        \n",
    "        # Connect file tool components to temporary storage\n",
    "        file_upload.change(\n",
    "            fn=process_file,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_message]\n",
    "        )\n",
    "        \n",
    "        screenshot_btn.click(\n",
    "            fn=take_screenshot,\n",
    "            inputs=[],\n",
    "            outputs=[file_message]\n",
    "        )\n",
    "        \n",
    "        github_input.submit(\n",
    "            fn=process_github,\n",
    "            inputs=[github_input],\n",
    "            outputs=[file_message]\n",
    "        )\n",
    "        \n",
    "        gdrive_input.submit(\n",
    "            fn=process_gdrive,\n",
    "            inputs=[gdrive_input],\n",
    "            outputs=[file_message]\n",
    "        )\n",
    "        \n",
    "        # Connect voice input to message\n",
    "        audio_input.change(\n",
    "            fn=transcribe_voice,\n",
    "            inputs=[audio_input],\n",
    "            outputs=[msg]\n",
    "        )\n",
    "        \n",
    "        # Connect toggle buttons for file tools and mic\n",
    "        plus_btn.click(\n",
    "            fn=lambda visible: (gr.update(visible=not visible), not visible),\n",
    "            inputs=[file_tools_visible],\n",
    "            outputs=[file_tools, file_tools_visible]\n",
    "        )\n",
    "        \n",
    "        mic_btn.click(\n",
    "            fn=lambda visible: (gr.update(visible=not visible), not visible),\n",
    "            inputs=[audio_input_visible],\n",
    "            outputs=[audio_input, audio_input_visible]\n",
    "        )\n",
    "        \n",
    "        # Connect text input and send button\n",
    "        msg.submit(\n",
    "            fn=respond,\n",
    "            inputs=[msg, chatbot, celebrity_selector, voice_toggle, file_message],\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            fn=respond,\n",
    "            inputs=[msg, chatbot, celebrity_selector, voice_toggle, file_message],\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        # Connect other components\n",
    "        celebrity_selector.change(change_celebrity, [celebrity_selector, chatbot], chatbot)\n",
    "        clear.click(clear_chat_history, [celebrity_selector], chatbot)\n",
    "        \n",
    "        # Message for instructions\n",
    "        gr.Markdown(\"\"\"\n",
    "        ## Instructions\n",
    "        1. Choose a celebrity from the dropdown menu\n",
    "        2. Toggle voice if you want spoken responses\n",
    "        3. Click âž• to upload files, take screenshots, or add links from GitHub/Google Drive\n",
    "        4. Click ðŸŽ¤ to record a voice message\n",
    "        5. Type your message and press Send\n",
    "        6. Mentions of cities will automatically show images\n",
    "        7. Enjoy chatting with your selected celebrity!\n",
    "        \n",
    "        *Note: This app requires API keys to be set as environment variables:*\n",
    "        - `OPENAI_API_KEY` for Galinda and city images/voice transcription\n",
    "        - `ANTHROPIC_API_KEY` for Sabrina Carpenter (if available)\n",
    "        - `ELEVENLABS_API_KEY` for voice generation (optional)\n",
    "        \"\"\")\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# For use in Jupyter Lab, run this cell\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your API keys here (for Jupyter Lab testing)\n",
    "    # Uncomment and fill in your API keys\n",
    "    \"\"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key-here\"\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key-here\"\n",
    "    os.environ[\"ELEVENLABS_API_KEY\"] = \"your-elevenlabs-key-here\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import safety check - print helpful error messages for missing dependencies\n",
    "    try:\n",
    "        import langchain\n",
    "        print(\"âœ“ LangChain is installed\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ LangChain is not installed. Run: pip install langchain\")\n",
    "    \n",
    "    try:\n",
    "        import openai\n",
    "        print(\"âœ“ OpenAI is installed\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ OpenAI is not installed. Run: pip install openai\")\n",
    "    \n",
    "    # Print versions for debugging\n",
    "    try:\n",
    "        print(f\"OpenAI version: {openai.__version__}\")\n",
    "        print(f\"LangChain version: {langchain.__version__}\")\n",
    "    except:\n",
    "        print(\"Could not determine package versions\")\n",
    "    \n",
    "    # Create and launch the chatbot\n",
    "    try:\n",
    "        demo = create_celebrity_chatbot()\n",
    "        if demo is not None:\n",
    "            demo.launch()\n",
    "        else:\n",
    "            print(\"Error: demo object is None\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error launching the chatbot: {e}\")\n",
    "        print(\"Try installing the required packages with:\")\n",
    "        print(\"pip install gradio langchain openai langchain-openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f211f777-3223-4c7b-8c0d-79dc1d306367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c93b188-2875-46f2-ade8-f6e4af434879",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cff39722-4055-49f1-86a0-aeb8590afeaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Error in request. Please check your input.', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m image = \u001b[43martist\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNew York City\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m display(image)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36martist\u001b[39m\u001b[34m(city)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34martist\u001b[39m(city):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     image_response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdall-e-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAn image representing a vacation in \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcity\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, showing tourist spots and everything unique about \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcity\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, in a vibrant pop-art style\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m512x512\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb64_json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     image_base64 = image_response.data[\u001b[32m0\u001b[39m].b64_json\n\u001b[32m     10\u001b[39m     image_data = base64.b64decode(image_base64)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\envs\\llms\\Lib\\site-packages\\openai\\resources\\images.py:264\u001b[39m, in \u001b[36mImages.generate\u001b[39m\u001b[34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    207\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    222\u001b[39m ) -> ImagesResponse:\n\u001b[32m    223\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03m    Creates an image given a prompt.\u001b[39;00m\n\u001b[32m    225\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    262\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/images/generations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquality\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstyle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageGenerateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mImagesResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:1276\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1264\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1272\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1273\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1274\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1275\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:949\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1054\u001b[39m         err.response.read()\n\u001b[32m   1056\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1060\u001b[39m     cast_to=cast_to,\n\u001b[32m   1061\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1065\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1066\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Error in request. Please check your input.', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "image = artist(\"New York City\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae518a06-210a-4168-8b00-bbdf57a0e11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
